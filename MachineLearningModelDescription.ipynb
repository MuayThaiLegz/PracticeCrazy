{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1gQnyYe83nx8sYYl0KOZ_UffDqtSQ8xdB",
      "authorship_tag": "ABX9TyM2zRwGPmwOcsdAg5nJqyTf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuayThaiLegz/PracticeCrazy/blob/main/MachineLearningModelDescription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install xformers\n",
        "# !pip install transformers\n",
        "# !pip install torch\n",
        "# !pip install --upgrade accelerate\n"
      ],
      "metadata": {
        "id": "5fFtT-LZo5-r"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize \n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# def preprocess_text(text):\n",
        "#     # Tokenization\n",
        "#     tokens = word_tokenize(text)\n",
        "    \n",
        "#     # Remove stopwords\n",
        "#     stop_words = set(stopwords.words('english'))\n",
        "#     tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "    \n",
        "#     # Lowercasing\n",
        "#     tokens = [token.lower() for token in tokens]\n",
        "    \n",
        "#     # Lemmatization\n",
        "#     lemmatizer = WordNetLemmatizer()\n",
        "#     tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "#     # Join tokens back into a single string\n",
        "#     preprocessed_text = ' '.join(tokens)\n",
        "    \n",
        "#     return preprocessed_text"
      ],
      "metadata": {
        "id": "7hMFeuhx-ZkG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'Linear Regression': ['A model that assumes a linear relationship between input features and the output. Suitable for regression tasks.',\n",
        "                          'Linear regression is also a type of machine-learning algorithm more specifically a supervised machine-learning algorithm that learns from the labeled datasets and maps the data points to the most optimized linear functions. which can be used for prediction on new datasets.',\n",
        "                          'Linear regression is a foundational statistical modeling technique that plays a critical role in data analysis and prediction. It focuses on understanding the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. The resulting equation allows for the estimation of the dependent variable based on the values of the independent variables. The primary objective of linear regression is to find the best-fit line that minimizes the differences between the observed data points and the predicted values generated by the linear equation. This best-fit line is determined by estimating the coefficients or weights assigned to each independent variable, which represent the change in the dependent variable for a unit change in the corresponding independent variable while holding other variables constant. Linear regression assumes a linear relationship between the dependent variable and the independent variables, meaning that changes in the independent variables lead to proportional changes in the dependent variable. This assumption provides a simplified but powerful framework for modeling and analyzing various real-world phenomena.'],\n",
        "   'Logistic Regression': ['A variation of Linear Regression, suitable for binary classification problems.',\n",
        "                           'Logistic regression is a supervised machine learning algorithm mainly used for classification tasks where the goal is to predict the probability that an instance of belonging to a given class. It is used for classification algorithms its name is logistic regression. itâ€™s referred to as regression because it takes the output of the linear regression function as input and uses a sigmoid function to estimate the probability for the given class. The difference between linear regression and logistic regression is that linear regression output is the continuous value that can be anything while logistic regression predicts the probability that an instance belongs to a given class or not.',\n",
        "                           'Logistic regression is a statistical modeling technique used to analyze the relationship between a binary or categorical dependent variable and one or more independent variables. It is specifically designed for situations where the outcome variable is not continuous but falls into distinct categories, such as yes/no, true/false, or success/failure. Logistic regression allows for the estimation of the probability of an event occurring based on the values of the independent variables.The primary goal of logistic regression is to model the relationship between the independent variables and the probability of an event or outcome. It accomplishes this by applying a logistic or sigmoid function to a linear combination of the independent variables, transforming the linear equation into a range of values between 0 and 1. This transformed value represents the predicted probability of the event occurring.'],\n",
        "    'Decision Trees': ['A model that breaks down the dataset into smaller subsets while incrementally developing a decision tree from decisions made about feature values.','A decision tree is a hierarchical model used in decision support that depicts decisions and their potential outcomes, incorporating chance events, resource expenses, and utility. This algorithmic model utilizes conditional control statements and is non-parametric, supervised learning, useful for both classification and regression tasks. The tree structure is comprised of a root node, branches, internal nodes, and leaf nodes, forming a hierarchical, tree-like structure.',\n",
        "                       'Decision trees are versatile and powerful machine learning models used for both classification and regression tasks. They represent a hierarchical structure that recursively partitions the data based on the values of different features or attributes, ultimately leading to the prediction of a target variable. The decision tree model is built using a top-down approach, where the root node represents the entire dataset. At each level of the tree, a decision is made based on a selected feature that best splits the data, resulting in subsets of data being directed to different branches. This splitting process continues recursively until the termination conditions are met, such as reaching a predefined depth, achieving a certain number of samples in the leaf nodes, or when no further improvement in prediction accuracy can be achieved.'],   \n",
        "    'Random Forest': ['An ensemble learning method that operates by constructing multiple decision trees at training time and outputting the class that is the mode of the classes or the mean prediction of the individual trees.',\n",
        "                      'Random Forest is one of the most popular and commonly used algorithms by Data Scientists. Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression. One of the most important features of the Random Forest Algorithm is that it can handle the data set containing continuous variables, as in the case of regression, and categorical variables, as in the case of classification. It performs better for classification and regression tasks. In this tutorial, we will understand the working of random forest and implement random forest on a classification task.',\n",
        "                      'Random Forest is a powerful ensemble learning method that combines multiple decision trees to make accurate predictions and improve generalization. It is widely used for both classification and regression tasks, providing robust and reliable results in various domains. At its core, a Random Forest consists of a collection or an ensemble of decision trees. Each tree in the ensemble is trained independently on a random subset of the data, called a bootstrap sample, created by randomly selecting data points with replacement from the original dataset. Additionally, for each split in a decision tree, only a random subset of features is considered, which helps to introduce diversity among the trees.'],\n",
        "    'Support Vector Machines': ['A representation of the training data as points in space, separated into categories by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap they fall on.',\n",
        "                              'A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, theyâ€™re able to categorize new text.',\n",
        "                              'Support Vector Machines (SVM) are powerful supervised machine learning models used for classification and regression tasks. SVMs are particularly effective in scenarios where the data is not linearly separable and requires nonlinear decision boundaries to make accurate predictions. At the heart of SVM is the concept of finding an optimal hyperplane that maximally separates the data points belonging to different classes. The hyperplane is selected in a way that maximizes the margin, which represents the distance between the hyperplane and the closest data points from each class, also known as support vectors. This margin maximization allows for better generalization and robustness of the model.'],\n",
        "    'Neural Networks': ['A model inspired by the human brain, capable of learning complex patterns.','Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another. Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.',\n",
        "                        'The fundamental building block of a Neural Network is the neuron or node, which receives input signals, processes them through an activation function, and generates an output signal. Neurons are organized into layers, with an input layer for receiving the input data, one or more hidden layers for intermediate processing, and an output layer for producing the final predictions or outputs. Training a Neural Network involves a process called forward propagation, where the input data is fed into the network, and the signals pass through the layers, undergoing weighted transformations and activations. The weights and biases associated with each neuron are adjusted iteratively during training using an optimization algorithm, such as gradient descent, to minimize the difference between the predicted outputs and the true labels or target values. This process is known as backpropagation, where the error is propagated backward through the network to update the weights.'],\n",
        "    'K-Nearest Neighbors': ['An instance-based learning algorithm, that classifies a new instance based on its similarity to existing instances.','The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.',\n",
        "                            'The KNN algorithm works by storing the entire training dataset in memory and using it as the knowledge base for making predictions. When a new data point is presented for prediction, the algorithm measures its proximity or similarity to the existing data points in the feature space. This proximity is typically determined using distance metrics such as Euclidean distance, Manhattan distance, or cosine similarity. In the case of classification, KNN assigns the majority class label among its K nearest neighbors to the new data point. The value of K is a hyperparameter that determines the number of neighbors considered. A larger K value leads to a smoother decision boundary but may increase the risk of misclassification. In regression, KNN calculates the average or weighted average of the target values of the K nearest neighbors as the predicted value for the new data point.'],\n",
        "    'Autoencoder': ['An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation.','Autoencoders, primarily used for dimensionality reduction and feature extraction, find use in anomaly detection due to their ability to reconstruct input data. Trained on non-anomalous data, an Autoencoder will accurately reconstruct these instances, while failing to do the same for anomalous data. Instances with a reconstruction error above a defined threshold can be classified as anomalies. This makes Autoencoders effective in scenarios where the goal is to detect outliers that significantly deviate from the norm.',\n",
        "                    'The architecture of an autoencoder typically consists of an encoder and a decoder. The encoder part of the network compresses the input data into a lower-dimensional representation, while the decoder aims to reconstruct the original input from this compressed representation. The encoder and decoder are trained together to minimize the reconstruction error, encouraging the network to learn meaningful and compact representations of the data. During training, the autoencoder is fed with input data, and the output of the decoder is compared to the original input using a loss function, such as mean squared error (MSE). Backpropagation is then used to update the weights and biases of the network, iteratively improving the reconstruction performance.'],\n",
        "    'Voting Classifier': ['An ensemble machine learning model that combines the predictions from multiple other models. It uses majority voting: the prediction label is the one that gets the majority of votes from models.','A voting classifier is a machine learning estimator that trains various base models or estimators and predicts on the basis of aggregating the findings of each base estimator. The aggregating criteria can be combined decision of voting for each estimator output.',\n",
        "                          'A Voting Classifier, also known as an Ensemble Classifier, is a machine learning model that combines the predictions of multiple individual classifiers to make a final prediction. It leverages the wisdom of the crowd by aggregating the predictions of diverse classifiers, resulting in improved overall performance and more robust predictions. The idea behind a Voting Classifier is that different classifiers may have varying strengths and weaknesses, and by combining their predictions, we can overcome individual shortcomings and obtain a more reliable and accurate prediction. The individual classifiers, often referred to as base or component classifiers, can be of different types, such as decision trees, support vector machines, logistic regression, or any other classifier suitable for the given problem.'],\n",
        "    'Grid Search': ['A hyperparameter tuning technique used to find the optimal hyperparameters for a model. It exhaustively tries every combination of the provided hyper-parameter values in order to find the best model.', 'Grid search is a process that searches exhaustively through a manually specified subset of the hyperparameter space of the targeted algorithm. Random search, on the other hand, selects a value for each hyperparameter independently using a probability distribution.', \n",
        "                    'Grid Search is a hyperparameter tuning technique used in machine learning to systematically search through a predefined set of hyperparameter combinations and determine the optimal configuration that yields the best model performance. It is a widely used method to fine-tune model parameters and improve the overall performance of machine learning models. Hyperparameters are settings that are not learned from the data but are set prior to training the model. They control the behavior and complexity of the model, such as the learning rate, regularization strength, number of hidden layers in a neural network, or the number of neighbors in a KNN algorithm. Selecting appropriate values for hyperparameters is crucial for achieving the best possible model performance. The Grid Search technique exhaustively explores a predefined grid or search space of hyperparameter values. It systematically evaluates the performance of the model using each combination of hyperparameters by performing cross-validation or using a separate validation set. The performance metric, such as accuracy or mean squared error, is calculated for each hyperparameter combination, and the optimal configuration is selected based on the highest performance metric.'],\n",
        "    'Isolation Forest': ['An unsupervised learning algorithm for anomaly detection that works on the principle of isolating anomalies, instead of the most common techniques of profiling normal points.', \"Isolation forest is a machine learning algorithm for anomaly detection. It's an unsupervised learning algorithm that identifies anomaly by isolating outliers in the data. Isolation Forest is based on the Decision Tree algorithm.\",\n",
        "                         'Isolation Forest is an unsupervised machine learning algorithm used for anomaly detection. It is particularly effective in identifying outliers or anomalies within a dataset, distinguishing them from the majority of normal data points. The algorithm constructs an ensemble of isolation trees to isolate anomalous instances in a quick and efficient manner.The Isolation Forest algorithm takes advantage of the fact that anomalies are typically fewer in number and exhibit different patterns compared to normal data points. It works by randomly selecting a feature and a random split value within the range of that feature. The feature is chosen randomly to prevent biased splits. The data is then partitioned based on whether the feature value of a data point is below or above the chosen split value. This partitioning process continues recursively, creating a tree-like structure until all data points are isolated or a predefined stopping criterion is met.'],\n",
        "    'Local Outlier Factor (LOF)': ['An unsupervised outlier detection method which computes the local density deviation of a given data point with respect to its neighbors.','The local outlier factor is based on a concept of a local density, where locality is given by k nearest neighbors, whose distance is used to estimate the density. By comparing the local density of an object to the local densities of its neighbors, one can identify regions of similar density, and points that have a substantially',\n",
        "                                   'Local Outlier Factor (LOF) is an unsupervised machine learning algorithm used for anomaly detection. It is a density-based method that identifies anomalies by measuring the local deviation of data points from their surrounding neighborhoods. LOF takes into account the density distribution of data points to identify outliers that deviate significantly from their local context. The LOF algorithm assesses the anomaly score of each data point based on the concept of local reachability density. For each data point, LOF calculates its local density by estimating the number of neighboring points within a defined distance (known as the neighborhood size). The local reachability density is then computed by comparing the local density of the data point with the local densities of its neighboring points. If a data point has a significantly lower local density compared to its neighbors, it suggests that the point is in a sparser region and could be an outlier.'],\n",
        "                \n",
        "    'One-Class SVM': ['An unsupervised algorithm that learns a decision function for novelty detection, classifying between observed and unobserved examples.','One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set.',\n",
        "                      'One-Class Support Vector Machines (One-Class SVM) is a machine learning algorithm used for unsupervised anomaly detection. It aims to identify anomalies by learning a boundary that encompasses the normal data points while separating them from potential outliers or anomalies. One-Class SVM is particularly useful when only normal data samples are available during training, making it suitable for situations where labeled anomalies are scarce or not available at all. One-Class SVM works by mapping the input data into a high-dimensional feature space and finding a hyperplane that encloses the majority of the data points. The algorithm finds the optimal hyperplane by maximizing the margin around the normal data instances while minimizing the number of data points that lie outside the boundary. In this way, One-Class SVM can capture the characteristics of normal data and identify deviations from it.']}        \n",
        "                            \n",
        "df = pd.DataFrame.from_dict(data, orient='index', columns=['Description', 'DescriptionLvl2', 'DescriptionLvl3'])\n",
        "                                    \n",
        "                              \n"
      ],
      "metadata": {
        "id": "vGPq4U25uzTl"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_continuous = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    data_continuous.append(f\"{index}\\n{row['Description']}\\n{row['DescriptionLvl2']}\\n{row['DescriptionLvl3']}\\n\")\n",
        "\n",
        "with open('data.txt', 'w') as file:\n",
        "    file.write(\"\\n\".join(data_continuous))\n"
      ],
      "metadata": {
        "id": "S1wi0_V5Pk4x"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "5M0jbiaQsifd"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"
      ],
      "metadata": {
        "id": "SDtgHGwVRhxJ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"data.txt\", # located in the root directory\n",
        "    block_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJO8c7sER1t9",
        "outputId": "53ddb9ad-90fb-4286-e11a-1e7adbd4cf36"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, \n",
        "    mlm=False)"
      ],
      "metadata": {
        "id": "9KLNYXalu697"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-model\", # output directory in root\n",
        "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
        "    num_train_epochs=100, # number of training epochs\n",
        "    learning_rate=5e-7, # learning rate\n",
        "    per_device_train_batch_size=4, # batch size for training\n",
        "    save_steps=10_000, # after # steps model is saved \n",
        "    save_total_limit=2, # delete older checkpoints\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "2eAFRZ-VQxvB"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "wVmXn-VLvOoS",
        "outputId": "8d6e7b86-bda0-4173-ec4c-76f254f01830"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2900' max='2900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2900/2900 04:11, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.892700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.576700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>3.405800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>3.296500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>3.248200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2900, training_loss=3.4476730030980605, metrics={'train_runtime': 252.0567, 'train_samples_per_second': 44.831, 'train_steps_per_second': 11.505, 'total_flos': 184537497600000.0, 'train_loss': 3.4476730030980605, 'epoch': 100.0})"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()\n"
      ],
      "metadata": {
        "id": "4wQhQhq5xPtb"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "headliner = pipeline('text-generation',\n",
        "                model='gpt2-model', \n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=100,\n",
        "                )\n",
        "def get_mlinfo(headliner_pipeline, seed_text=None):\n",
        "  return headliner_pipeline(seed_text)[0]['generated_text']"
      ],
      "metadata": {
        "id": "IGTtbCi9vvJM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_mlinfo(headliner, seed_text=\"Autoencoder\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "VLA6OsOyw_25",
        "outputId": "ac9023ff-3851-47f6-be81-f01203868aa8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Autoencoder and Recurrent Neural Network (RSNN) for supervised learning algorithms, has been applied to a wide range of data points and is able to identify complex interactions between groups of the data. It can then perform highly accurate task representations in real-time. Recurrent Neural Networks have been used in many different industries, such as computer vision, neural classification, natural language processing, automatic classification and machine learning systems. Recurrent Neural Networks are particularly popular as they have the potential to deliver new data sets as part'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Support Vector Machines\"\n",
        "input_ids = tokenizer.encode(input_text)\n",
        "\n",
        "# generate text\n",
        "output = model.generate(input_ids, max_length=150, num_return_sequences=1, temperature=0.9)\n",
        "\n",
        "for i in range(5):\n",
        "    print(tokenizer.decode(output[i], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "gDcihK6VRgHs"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YzeGgEMjx0fW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
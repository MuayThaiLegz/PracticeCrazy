{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1gQnyYe83nx8sYYl0KOZ_UffDqtSQ8xdB",
      "authorship_tag": "ABX9TyNQ1p9P+nVpwcMo/CAh/aVn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuayThaiLegz/PracticeCrazy/blob/main/MachineLearningModelDescription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xformers\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install --upgrade accelerate\n",
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fFtT-LZo5-r",
        "outputId": "4a4397cc-8aa0-463a-b8dc-86f81c19a0fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xformers in /usr/local/lib/python3.10/dist-packages (0.0.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.22.4)\n",
            "Requirement already satisfied: pyre-extensions==0.0.29 in /usr/local/lib/python3.10/dist-packages (from xformers) (0.0.29)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: typing-inspect in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->xformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->xformers) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->xformers) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->xformers) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect->pyre-extensions==0.0.29->xformers) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize \n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n"
      ],
      "metadata": {
        "id": "7hMFeuhx-ZkG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qIf9pTYvB-S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "        'Missing Values' : [\n",
        "        \n",
        "        \"\"\"\n",
        "        Typically, some records will contain missing values. If the number of records with missing values is small, those records might be omitted. However, if we have a large number of variables, even a small proportion of missing values can affect a lot of records. Even with only 30 variables, if only 5% of the values are missing (spread randomly and independently among cases and variables), almost 80% of the records would have to be omitted from the analysis. (The chance that a given record would escape having a missing value is 0.95 30 = 0.215.) \\n\n",
        "        An alternative to omitting records with missing values is to replace the missing value with an imputed value, based on the other values for that variable across all records. records. For example, if among 30 variables, household income is missing for a particular record, we might substitute the mean household income across all records. Doing so does not, of course, add any information about how household income affects the outcome variable. It merely allows us to proceed with the analysis and not lose the information contained in this record for the other 29 variables.\\n\n",
        "        Note that using such a technique will understate the variability in a dataset. However, we can assess variability and the performance of our data mining technique using the validation data, and therefore this need not present a major problem. One option is to replace missing values using fairly simple substitutes (e.g., mean, median). More sophisticated procedures do exist—for example, using linear regression, based on other variables, to fill in the missing values. These methods have been elaborated mainly for analysis of medical and scientific studies, where each patient or subject record comes at great expense. In data mining, where data are typically plentiful, simpler methods usually suffice. purposes. The median is used for imputation, rather than the mean, to preserve the integer nature of the counts\\n\n",
        "        Some datasets contain variables that have a very large number of missing values. In other words, a measurement is missing for a large number of records. In that case, dropping records with missing values will lead to a large loss of data. Imputing the missing values might also be useless, as the imputations are based on a small number of existing records. An alternative is to examine the importance of the predictor. If it is not very crucial, it can be dropped. If it is important, perhaps a proxy variable with fewer missing values can be used instead. When such a predictor is deemed central, the best solution is to invest in obtaining the missing data.\\n\n",
        "        Significant time may be required to deal with missing data, as not all situations are susceptible to automated solutions. In a messy dataset, for example, a “0” might mean two things: (1) the value is missing, or (2) the value is actually zero. Human judgment may be required for individual cases or to determine a special rule to deal with the situation.\\n\n",
        "        \"\"\"\n",
        "        ],        \n",
        "        \n",
        "    'Normalizing (Standardizing) and Rescaling Data' : [\n",
        "        \n",
        "        \"\"\"\n",
        "        Some algorithms require that the data be normalized before the algorithm can be implemented effectively. To normalize a variable, we subtract the mean from each value and then divide by the standard deviation.\n",
        "        This operation is also sometimes called standardizing. pandas has no custom method for this, however, the operation can be easily performed using the methods mean and std; (df -df.mean()) / df.std(). In effect, we are expressing each value as the “number of standard deviations away from the mean,” also called a z-score. An alternative is the class StandardScaler(), which is one of a number different transformers available in scikit-learn.\n",
        "        You can use the methods fit() or fit_transform() to train the transformer on the training set and the method transform() to apply on the validation set. The result of the transformation is no longer a pandas dataframe, however, you can convert it back into one\n",
        "        Normalizing is one way to bring all variables to the same scale. Another popular approach is rescaling each variable to a [0, 1] scale. This is done by subtracting the minimum value and then dividing by the range. Subtracting the minimum shifts the variable origin to zero. Dividing by the range shrinks or expands the data to the range [0, 1]. In pandas, use the expression (df-df.min())/ (df.max()-df.min()). The corresponding scikit-learn transformer is MinMaxScaler.\n",
        "        To consider why normalizing or scaling to [0, 1] might be necessary, consider the case of clustering. Clustering typically involves calculating a distance measure that reflects how far each record is from a cluster center or from other records. With multiple variables, different units will be used: days, dollars, counts, and so on. If the dollars are in the thousands and everything else is in the tens, the dollar variable will come to dominate the distance measure. Moreover, changing units from, say, days to hours or months, could alter the outcome completely.\n",
        "        \"\"\"\n",
        "        ],\n",
        "        \n",
        "    'Grid Search': [\n",
        "        \n",
        "        \"\"\"\n",
        "        When the number of records in our sample is small, data partitioning might not be advisable as each partition will contain too few records for model building and performance evaluation. Furthermore, some data mining methods are sensitive to small changes in the training data, so that a different partitioning can lead to different results.\n",
        "        An alternative to data partitioning is cross-validation, which is especially useful with small samples. Cross-validation, or k-fold cross-validation, is a procedure that starts with partitioning the data into “folds,” or non-overlapping subsamples. Often we choose k = 5 folds, meaning that the data are randomly partitioned into five equal parts, where each fold has 20% of the observations. A model is then fit k times. Each time, one of the folds is used as the validation set and the remaining k − 1 folds serve as the training set. The result is that each fold is used once as the validation set, thereby producing predictions for every observation in the dataset.\n",
        "        A hyperparameter tuning technique used to find the optimal hyperparameters for a model. It exhaustively tries every combination of the provided hyper-parameter values in order to find the best model. Grid search is a process that searches exhaustively through a manually specified subset of the hyperparameter space of the targeted algorithm. Random search, on the other hand, selects a value for each hyperparameter independently using a probability distribution.\n",
        "        Grid Search is a hyperparameter tuning technique used in machine learning to systematically search through a predefined set of hyperparameter combinations and determine the optimal configuration that yields the best model performance. It is a widely used method to fine-tune model parameters and improve the overall performance of machine learning models. Hyperparameters are settings that are not learned from the data but are set prior to training the model. They control the behavior and complexity of the model, such as the learning rate, regularization strength, number of hidden layers in a neural network, or the number of neighbors in a KNN algorithm. Selecting appropriate values for hyperparameters is crucial for achieving the best possible model performance. The Grid Search technique exhaustively explores a predefined grid or search space of hyperparameter values. It systematically evaluates the performance of the model using each combination of hyperparameters by performing cross-validation or using a separate validation set. The performance metric, such as accuracy or mean squared error, is calculated for each hyperparameter combination, and the optimal configuration is selected based on the highest performance metric.\n",
        "        We can then combine the models predictions on each of the k validation sets in order to evaluate the overall performance of the model. In Python, cross-validation is achieved using the cross_val_score() or the more general cross_validate function, where argument cv determines the number of folds. Sometimes cross validation is built into a data mining algorithm, with the results of the cross-validation used for choosing the algorithm’s parameters'\n",
        "        \"\"\"\n",
        "        ],\n",
        "\n",
        "    'Supervised and Unsupervised Learning' : [\n",
        "        \n",
        "        \"\"\"\n",
        "        A fundamental distinction among data mining techniques is between supervised and unsupervised methods. Supervised learning algorithms are those used in classification and prediction. We must have data available in which the value of the outcome of interest (e.g., purchase or no purchase) is known. Such data are also called “labeled data,” since they contain the label (outcome value) for each record. These training data are the data from which the classification or prediction algorithm “learns,” or is “trained,” about the relationship between predictor variables and the outcome variable. Once the algorithm has learned from the training data, it is then applied to another sample of labeled data (the validation data) where the outcome is known but initially hidden, to see how well it does in comparison to other models.\n",
        "        If many different models are being tried out, it is prudent to save a third sample, which also includes known outcomes (the test data) to use with the model finally selected to predict how well it will do. The model can then be used to classify or predict the outcome of interest in new cases where the outcome is unknown. Simple linear regression is an example of a supervised learning algorithm (although rarely called that in the introductory statistics course where you probably first encountered it). The Y variable is the (known) outcome variable and the X variable is a predictor variable.\n",
        "        A regression line is drawn to minimize the sum of squared deviations between the actual Y values and the values predicted by this line. The regression line can now be used to predict Y values for new values of X for which we do not know the Y value. Unsupervised learning algorithms are those used where there is no outcome variable to predict or classify. Hence, there is no “learning” from cases where such an outcome variable is known. Association rules, dimension reduction methods, and clustering techniques are all unsupervised learning methods. Supervised and unsupervised methods are sometimes used in conjunction. For example, unsupervised clustering methods are used to separate loan applicants into several risk-level groups. Then, supervised algorithms are applied separately to each risk-level group for predicting propensity of loan default.\n",
        "        The scatter plot matrix is useful in unsupervised learning for studying the associations between numerical variables, detecting outliers and identifying clusters. For supervised learning, it can be used for examining pairwise relationships (and their nature) between predictors to support variable transformations and variable selection (see Correlation Analysis in Chapter 4). For prediction, it can also be used to depict the relationship of the outcome with the numerical predictors.\n",
        "        Deep learning is a rapidly growing and evolving field, with many aspects that distinguish it from simpler predictive models. A full examination of the field of deep learning is beyond the scope of this book, but let us consider one key innovation in the area of algorithmic design that is associated with deep learning, and extends the ability of simple neural networks to perform the tasks of supervised and unsupervised learning that we have been discussing.\n",
        "        \"\"\"\n",
        "        ],\n",
        "\n",
        "    'Variable Selection' : [\n",
        "        \n",
        "        \"\"\"\n",
        "        More is not necessarily better when it comes to selecting variables for a model. Other things being equal, parsimony, or compactness, is a desirable feature in a model. For one thing, the more variables we include and the more complex the model, the greater the number of records we will need to assess  relationships among the variables. Fifteen records may suffice to give us a rough idea of the relationship between Y and a single predictor variable X. If we now want information about the relationship between Y and 15 predictor variables X1, ..., X15, 15 records will not be enough (eachestimated relationship would have an average of only one record’s worth of information, making theestimate very unreliable). In addition, models based on many variables are often less robust, as they require the collection of more variables in the future, are subject to more data quality and availability issues, and require more data cleaning and preprocessing. Another rule, used by Delmaster and Hancockfor classification procedures, is to have at least 6 × m × p records, where m is the number of outcome classes and p is the number of variables.\n",
        "        Statisticians give us procedures to learn with some precision how many records we would need to achieve a given degree of reliability with a given dataset and a given model. These are called “power calculations” and are intended to assure that an average population effect will be estimated with sufficient precision from a sample.\n",
        "        Data miners, needs are usually different, because the focus is not on identifying an average effect but rather on predicting individual records. This purpose typically requires larger samples than those used for statistical inference. A good rule of thumb is to have 10records for every predictor variable.\n",
        "        In general, compactness or parsimony is a desirable feature in a data mining model. Even when we start with a small number of variables, we often end up with many more after creating new variables (such as converting a categorical variable into a set of dummy variables). Data visualization and dimension reduction methods help reduce the number of variables so that redundancies and information overlap are reduced.\n",
        "        Even when we have an ample supply of data, there are good reasons to pay close attention to thevariables that are included in a model. Someone with domain knowledge (i.e., knowledge of thebusiness process and the data) should be consulted, as knowledge of what the variables represent is typically critical for building a good model and avoiding errors.\n",
        "        \"\"\"\n",
        "        ],\n",
        "\n",
        "\n",
        "    'Outliers' : [\n",
        "        \n",
        "        \"\"\"\n",
        "        The more data we are dealing with, the greater the chance of encountering erroneous values resulting from measurement error, data-entry error, or the like. If the erroneous value is in the same range as the rest of the data, it may be harmless. If it is well outside the range of the rest of the data (e.g., a misplaced decimal), it may have a substantial effect on some of the data mining procedures we plan to use.\n",
        "        Values that lie far away from the bulk of the data are called outliers. The term far away is deliberately left vague because what is or is not called an outlier is an arbitrary decision. Analysts use rules of thumb such as “anything over three standard deviations away from the mean is an outlier,” but no statistical rule can tell us whether such an outlier is the result of an error. In this statistical sense, an outlier is not necessarily an invalid data point, it is just a distant one.\n",
        "        The purpose of identifying outliers is usually to call attention to values that need further review. We might come up with an explanation looking at the data—in the case of a misplaced decimal, this is likely. We might have no explanation, but know that the value is wrong. Or, we might conclude that the value is within the realm of possibility and leave it alone.\n",
        "        All these are judgments best made by someone with domain knowledge, knowledge of the particular application being considered: direct mail, mortgage finance, and so on, as opposed to technical knowledge of statistical or data mining procedures. Statistical procedures can do little beyond identifying the record as something that needs review.\n",
        "        If manual review is feasible, some outliers may be identified and corrected. In any case, if the number of records with outliers is very small, they might be treated as missing data. How do we inspect for outliers? One technique is to sort the records by the first column (e.g., using the pandas method sort_values() such as df.sort_values(by=[’col1’])), then review the data for very large or very small values in that column. Then repeat for each successive column. Another option is to examine the minimum and maximum values of each column using pandas’s min() and max() methods. For a more automated approach that considers each record as a unit, rather than each column in isolation, clustering techniques could be used to identify clusters of one or a few records that are distant from others. Those records could then be examined.\n",
        "        \"\"\"\n",
        "        ],\n",
        "\n",
        "    'Data Reduction and Dimension Reduction' : [\n",
        "        \n",
        "        \"\"\"\n",
        "        The performance of data mining algorithms is often improved when the number of variables is limited, and when large numbers of records can be grouped into homogeneous groups. For example, rather than dealing with thousands of product types, an analyst might wish to group them into a smaller number of groups and build separate models for each group. Or a marketer might want to classify customers into different “personas,” and must therefore group customers into homogeneous groups to define the personas.\n",
        "        This process of consolidating a large number of records (or cases) into a smaller set is termed data reduction. Methods for reducing the number of cases are often called clustering. Reducing the number of variables is typically called dimension reduction. Dimension reduction is a common initial step before deploying data mining methods, intended to improve predictive power, manageability, and interpretability.\n",
        "        Numerical summaries and graphs of the data are very helpful for data reduction. The information that they convey can assist in combining categories of a categorical variable, in choosing variables to remove, in assessing the level of information overlap between variables, and more. Before discussing such strategies for reducing the dimension of a data set, let us consider useful summaries and tools.\n",
        "        The pandas package offers several methods that assist in summarizing data. The DataFrame method describe() gives an overview of the entire set of variables in the data. The methods mean(), std(), min(), max(), median(), and len() are also very helpful for learning about the characteristics of each variable. First, they give us information about the scale and type of values that the variable takes.\n",
        "        The min and max statistics can be used to detect extreme values that might be errors. The mean and median give a sense of the central values of that variable, and a large deviation between the two also indicates skew. The standard deviation gives a sense of how dispersed the data are (relative to the mean). Further options, such as the combination of .isnull().sum(), which gives the number of null values, can tell us about missing values.\n",
        "        \"\"\"\n",
        "        ],\n",
        "        \n",
        "    'Data Exploration and Visualization' : [\n",
        "        \n",
        "        \"\"\"\n",
        "        One of the earliest stages of engaging with a dataset is exploring it. Exploration is aimed at understanding the global landscape of the data, and detecting unusual values. Exploration is used for data cleaning and manipulation as well as for visual discovery and “hypothesis generation.\n",
        "        Methods for exploring data include looking at various data aggregations and summaries, both numerically and graphically. This includes looking at each variable separately as well as looking at relationships among variables. The purpose is to discover patterns and exceptions. Exploration by creating charts and dashboards is called Data Visualization or Visual Analytics. For numerical variables, we use histograms and boxplots to learn about the distribution of their values, to detect outliers (extreme observations), and to find other information that is relevant to the analysis task.\n",
        "        Similarly, for categorical variables, we use bar charts. We can also look at scatter plots of pairs of numerical variables to learn about possible relationships, the type of relationship, and again, to detect outliers. Visualization can be greatly enhanced by adding features such as color and interactive navigation.\n",
        "        The oldest, but also most flexible library is matplotlib. It is widely used and there are many resources available to get started and to find help. A number of other libraries are built around it and make the generation of plots often quicker. Seaborn and pandas are two of the libraries that are wrappers around matplotlib. Both are very useful to create plots quickly. However, even if these are used, a good knowledge of matplotlib helps to have more control over the final plot.\n",
        "        Data exploration is a mandatory initial step whether or not more formal analysis follows. Graphical exploration can support free-form exploration for the purpose of understanding the data structure, cleaning the data (e.g., identifying unexpected gaps or “illegal” values), identifying outliers, discovering initial patterns (e.g., correlations among variables and surprising clusters), and generating interesting questions. Graphical exploration can also be more focused, geared toward specific questions of interest. In the data mining context, a combination is needed: free-form exploration performed with the purpose of supporting a specific goal.\n",
        "        \"\"\"\n",
        "        ],\n",
        "        \n",
        "        \n",
        "    'Linear Regression': [\n",
        "        \n",
        "        \"\"\"\n",
        "        A model that assumes a linear relationship between input features and the output. Suitable for regression tasks. Linear regression is also a type of machine-learning algorithm more specifically a supervised machine-learning algorithm that learns from the labeled datasets and maps the data points to the most optimized linear functions. which can be used for prediction on new datasets.\n",
        "        Linear regression is a foundational statistical modeling technique that plays a critical role in data analysis and prediction. It focuses on understanding the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the observed data. The resulting equation allows for the estimation of the dependent variable based on the values of the independent variables. The primary objective of linear regression is to find the best-fit line that minimizes the differences between the observed data points and the predicted values generated by the linear equation. This best-fit line is determined by estimating the coefficients or weights assigned to each independent variable, which represent the change in the dependent variable for a unit change in the corresponding independent variable while holding other variables constant. Linear regression assumes a linear relationship between the dependent variable and the independent variables, meaning that changes in the independent variables lead to proportional changes in the dependent variable. This assumption provides a simplified but powerful framework for modeling and analyzing various real-world phenomena.\n",
        "        linear regression models for the purpose of prediction. We discuss the differences between fitting and using regression models for the purpose of inference (as in classical statistics) and for prediction. A predictive goal calls for evaluating model performance on a validation set, and for using predictive metrics. We then raise the challenges of using many predictors and describe variable selection algorithms that are often implemented in linear regression procedures. The most popular model for making predictions is the multiple linear regression model encountered in most introductory statistics courses and textbooks. This model is used to fit a relationship between a numerical outcome variable Y (also called the response, target, or dependent variable) and a set of predictorsX1 , X2 , ..., Xp (also referred to as independent variables, input variables, regressors, or covariates).\n",
        "        Regression modeling means not only estimating the coefficients but also choosing which predictors to include and in what form. For example, a numerical predictor can be included as is, or in logarithmic form [log (X)], or in a binned form (e.g., age group). Choosing the right form depends on domain knowledge, data availability, and needed predictive power.\n",
        "        In predictive analytics, however, the focus is typically on the second goal: predicting new individual records. Here we are not interested in the coefficients themselves, nor in the “average record,” but rather in the predictions that this model can generate for new records. In this scenario, the model is used for micro-decision-making at the record level.\n",
        "        \"\"\"\n",
        "        ],\n",
        "   \n",
        "   'Logistic Regression': [\n",
        "      \n",
        "      \"\"\"\n",
        "      A variation of Linear Regression, suitable for binary classification problems. Logistic regression is a supervised machine learning algorithm mainly used for classification tasks where the goal is to predict the probability that an instance of belonging to a given class. It is used for classification algorithms its name is logistic regression. it’s referred to as regression because it takes the output of the linear regression function as input and uses a sigmoid function to estimate the probability for the given class. The difference between linear regression and logistic regression is that linear regression output is the continuous value that can be anything while logistic regression predicts the probability that an instance belongs to a given class or not.\n",
        "      Logistic regression is a statistical modeling technique used to analyze the relationship between a binary or categorical dependent variable and one or more independent variables. It is specifically designed for situations where the outcome variable is not continuous but falls into distinct categories, such as yes/no, true/false, or success/failure. Logistic regression allows for the estimation of the probability of an event occurring based on the values of the independent variables.The primary goal of logistic regression is to model the relationship between the independent variables and the probability of an event or outcome. It accomplishes this by applying a logistic or sigmoid function to a linear combination of the independent variables, transforming the linear equation into a range of values between 0 and 1. This transformed value represents the predicted probability of the event occurring.\n",
        "      The highly popular and powerful classification method called logistic regression. relies on a specific model relating the predictors with the outcome. The user must specify the predictors to include as well as their form (e.g., including any interaction terms). The logistic regression model is used in a variety of fields: whenever a structured model is needed to explain or predict categorical (in particular, binary) outcomes.\n",
        "      This means that even small datasets can be used for building logistic regression classifiers, and that once the model is estimated, it is computationally fast and cheap to classify even large samples of new records. Logistic regression extends the ideas of linear regression to the situation where the outcome variable, Y, is categorical. We can think of a categorical variable as dividing the records into classes.\n",
        "      Logistic regression can be used for classifying a new record, where its class is unknown, into one of the classes, based on the values of its predictor variables (called classification). It can also be used in data where the class is known, to find factors distinguishing between records in different classes in terms of their predictor variables, or “predictor profile” (called profiling).\n",
        "      \"\"\"\n",
        "        ],\n",
        "\n",
        "    'Decision Trees': [\n",
        "        \n",
        "        \"\"\"\n",
        "        A model that breaks down the dataset into smaller subsets while incrementally developing a decision tree from decisions made about feature values.','A decision tree is a hierarchical model used in decision support that depicts decisions and their potential outcomes, incorporating chance events, resource expenses, and utility. This algorithmic model utilizes conditional control statements and is non-parametric, supervised learning, useful for both classification and regression tasks. The tree structure is comprised of a root node, branches, internal nodes, and leaf nodes, forming a hierarchical, tree-like structure.\n",
        "        This chapter describes a flexible data-driven method that can be used for both classification (called classification tree) and prediction (called regression tree). Among the data-driven methods, trees are the most transparent and easy to interpret. Trees are based on separating records into subgroups by creating splits on predictors. These splits create logical rules that are transparent and easily understandable, for example, “IF Age < 55 AND Education > 12 THEN class = 1.”.\n",
        "        The resulting subgroups should be more homogeneous in terms of the outcome variable, thereby creating useful prediction or classification rules. We discuss the two key ideas underlying trees: recursive partitioning (for constructing the tree) and pruning (for cutting the tree back). In the context of tree construction, we also describe a few metrics of homogeneity that are popular in tree algorithms, for determining the homogeneity of the resulting subgroups of records. We explain that limiting tree size is a useful strategy for avoiding overfitting and show how it is done.\n",
        "        Decision trees are versatile and powerful machine learning models used for both classification and regression tasks. They represent a hierarchical structure that recursively partitions the data based on the values of different features or attributes, ultimately leading to the prediction of a target variable. The decision tree model is built using a top-down approach, where the root node represents the entire dataset. At each level of the tree, a decision is made based on a selected feature that best splits the data, resulting in subsets of data being directed to different branches. This splitting process continues recursively until the termination conditions are met, such as reaching a predefined depth, achieving a certain number of samples in the leaf nodes, or when no further improvement in prediction accuracy can be achieved.\n",
        "        We have two types of nodes in a tree: decision (=splitting) nodes and terminal nodes. Nodes that have successors are called decision nodes because if we were to use a tree to classify a new record for which we knew only the values of the predictor variables, we would “drop” the record down the tree so that at each decision node, the appropriate branch is taken until we get to a node that has no successors. Such nodes are called the terminal nodes (or leaves of the tree), and represent the partitioning of the data by predictors.\n",
        "        \"\"\"\n",
        "        ],\n",
        "\n",
        "    'Random Forest': [\n",
        "        \"\"\"\n",
        "        An ensemble learning method that operates by constructing multiple decision trees at training time and outputting the class that is the mode of the classes or the mean prediction of the individual trees. The basic idea in random forests is to Draw multiple random samples, with replacement, from the data (this sampling approach is called the bootstrap). Using a random subset of predictors at each stage, fit a classification (or regression) tree to eachsample (and thus obtain a “forest”). 3. Combine the predictions/classifications from the individual trees to obtain improved predictions. Use voting for classification and averaging for prediction.\n",
        "        Random Forest is one of the most popular and commonly used algorithms by Data Scientists. Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression. One of the most important features of the Random Forest Algorithm is that it can handle the data set containing continuous variables, as in the case of regression, and categorical variables, as in the case of classification. It performs better for classification and regression tasks. In this tutorial, we will understand the working of random forest and implement random forest on a classification task.\n",
        "        Random Forest is a powerful ensemble learning method that combines multiple decision trees to make accurate predictions and improve generalization. It is widely used for both classification and regression tasks, providing robust and reliable results in various domains. At its core, a Random Forest consists of a collection or an ensemble of decision trees. Each tree in the ensemble is trained independently on a random subset of the data, called a bootstrap sample, created by randomly selecting data points with replacement from the original dataset. Additionally, for each split in a decision tree, only a random subset of features is considered, which helps to introduce diversity among the trees.\n",
        "        Unlike a single tree, results from a random forest cannot be displayed in a tree-like diagram, thereby losing the interpretability that a single tree provides. However, random forests can produce “variable importance” scores, which measure the relative contribution of the different predictors. The importance score for a particular predictor is computed by summing up the decrease in the Gini index for that predictor over all the trees in the forest.\n",
        "        Tree methods are good off-the-shelf classifiers and predictors. They are also useful for variable selection, with the most important predictors usually showing up at the top of the tree. Trees require relatively little effort from users in the following senses: First, there is no need for transformation of variables (any monotone transformation of the variables will give the same trees). Second, variable subset selection is automatic since it is part of the split selection.\n",
        "        \"\"\"\n",
        "        ],\n",
        "\n",
        "    'Support Vector Machines': [\n",
        "          \"\"\"\n",
        "          A representation of the training data as points in space, separated into categories by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap they fall on. A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems. After giving an SVM model sets of labeled training data for each category, they’re able to categorize new text.\n",
        "          Support Vector Machines (SVM) are powerful supervised machine learning models used for classification and regression tasks. SVMs are particularly effective in scenarios where the data is not linearly separable and requires nonlinear decision boundaries to make accurate predictions. At the heart of SVM is the concept of finding an optimal hyperplane that maximally separates the data points belonging to different classes. The hyperplane is selected in a way that maximizes the margin, which represents the distance between the hyperplane and the closest data points from each class, also known as support vectors. This margin maximization allows for better generalization and robustness of the model.\n",
        "          Another powerful and widely used learning algorithm is the support vector machine (SVM), which can be considered an extension of the perceptron. Using the perceptron algorithm, we minimized misclassification errors. However, in SVMs our optimization objective is to maximize the margin.\n",
        "          The margin is defined as the distance between the separating hyperplane (decision boundary) and the training examples that are closest to this hyperplane, which are the so-called support vectors.\n",
        "          Another reason why SVMs enjoy high popularity among machine learning practitioners is that they can be easily kernelized to solve nonlinear classification problems. Before we discuss the main concept behind the so-called kernel SVM, the most common variant of SVMs,\n",
        "\n",
        "          \"\"\"\n",
        "          ],\n",
        "        \n",
        "        \n",
        "    'Neural Networks': [ \n",
        "        \"\"\"\n",
        "        A model inspired by the human brain, capable of learning complex patterns.','Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another. Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
        "        The fundamental building block of a Neural Network is the neuron or node, which receives input signals, processes them through an activation function, and generates an output signal. Neurons are organized into layers, with an input layer for receiving the input data, one or more hidden layers for intermediate processing, and an output layer for producing the final predictions or outputs. Training a Neural Network involves a process called forward propagation, where the input data is fed into the network, and the signals pass through the layers, undergoing weighted transformations and activations. The weights and biases associated with each neuron are adjusted iteratively during training using an optimization algorithm, such as gradient descent, to minimize the difference between the predicted outputs and the true labels or target values. This process is known as backpropagation, where the error is propagated backward through the network to update the weights.\n",
        "        The MLP depicted in the preceding figure has one input layer, one hidden layer, and one output layer. The units in the hidden layer are fully connected to the input layer, and the output layer is fully connected to the hidden layer. If such a network has more than one hidden layer, we also call it a deep artificial NN.\n",
        "        We can add any number of hidden layers to the MLP to create deeper network architectures. Practically, we can think of the number of layers and units in an NN as additional hyperparameters that we want to optimize for a given problem task using cross-validation technique, which we discussed in Chapter 6, Learning Best Practices for Model Evaluation and Hyperparameter Tuning. \n",
        "        However, the error gradients, which we will calculate later via backpropagation, will become increasingly small as more layers are added to a network. This vanishing gradient problem makes the model learning more challenging. Therefore, special algorithms have been developed to help train such DNN structures; this is known as deep learning.\n",
        "        \"\"\"\n",
        "        ],\n",
        "\n",
        "    'K-Nearest Neighbors': [\n",
        "        \"\"\"\n",
        "        An instance-based learning algorithm, that classifies a new instance based on its similarity to existing instances.','The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.\n",
        "        The KNN algorithm works by storing the entire training dataset in memory and using it as the knowledge base for making predictions. When a new data point is presented for prediction, the algorithm measures its proximity or similarity to the existing data points in the feature space. This proximity is typically determined using distance metrics such as Euclidean distance, Manhattan distance, or cosine similarity. In the case of classification, KNN assigns the majority class label among its K nearest neighbors to the new data point. The value of K is a hyperparameter that determines the number of neighbors considered. A larger K value leads to a smoother decision boundary but may increase the risk of misclassification. In regression, KNN calculates the average or weighted average of the target values of the K nearest neighbors as the predicted value for the new data point.\n",
        "        The KNN algorithm itself is fairly straightforward and can be summarized by the following steps: 1. Choose the number of k and a distance metric. 2. Find the k-nearest neighbors of the data record that we want to classify. 3. Assign the class label by majority vote. Based on the chosen distance metric, the KNN algorithm finds the k examples in the training dataset that are closest (most similar) to the point that we want to classify. The class label of the data point is then determined by a majority vote among its k nearest neighbors.\n",
        "        The main advantage of such a memory-based approach is that the classifier immediately adapts as we collect new training data. However, the downside is that the computational complexity for classifying new examples grows linearly with the number of examples in the training dataset in the worst-case scenario—unless the dataset has very few dimensions (features) and the algorithm has been implemented using efficient data structures such as k-d trees Furthermore, we can't discard training examples since no training step is involved. Thus, storage space can become a challenge if we are working with large datasets.\n",
        "        The right choice of k is crucial to finding a good balance between overfitting and underfitting. We also have to make sure that we choose a distance metric that is appropriate for the features in the dataset. Often, a simple Euclidean distance measure is used for real-value examples, for example, the flowers in our Iris dataset, which have features measured in centimeters. However, if we are using a Euclidean distance measure, it is also important to standardize the data so that each feature contributes equally to the distance. The minkowski distance that we used in the previous code is just a generalization of the Euclidean and Manhattan distance.\n",
        "        \"\"\"\n",
        "        ],\n",
        "\n",
        "    'Autoencoder': [\n",
        "        \"\"\"\n",
        "        An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). An autoencoder learns two functions: an encoding function that transforms the input data, and a decoding function that recreates the input data from the encoded representation.','Autoencoders, primarily used for dimensionality reduction and feature extraction, find use in anomaly detection due to their ability to reconstruct input data. Trained on non-anomalous data, an Autoencoder will accurately reconstruct these instances, while failing to do the same for anomalous data. Instances with a reconstruction error above a defined threshold can be classified as anomalies. This makes Autoencoders effective in scenarios where the goal is to detect outliers that significantly deviate from the norm.\n",
        "        The architecture of an autoencoder typically consists of an encoder and a decoder. The encoder part of the network compresses the input data into a lower-dimensional representation, while the decoder aims to reconstruct the original input from this compressed representation. The encoder and decoder are trained together to minimize the reconstruction error, encouraging the network to learn meaningful and compact representations of the data. During training, the autoencoder is fed with input data, and the output of the decoder is compared to the original input using a loss function, such as mean squared error (MSE). Backpropagation is then used to update the weights and biases of the network, iteratively improving the reconstruction performance.\n",
        "        Autoencoders are composed of two networks concatenated together: an encoder network and a decoder network. The encoder network receives a d-dimensional input feature vector associated with example x (that is, xx ∈ RRdd) and encodes it into a p-dimensional vector, z (that is, zz ∈ RRpp). In other words, the role of the encoder is to learn how to model the function zz = ff(xx). The encoded vector, z, is also called the latent vector, or the latent feature representation. Typically, the dimensionality of the latent vector is less than that of the input examples; in other words, p < d. Hence, we can say that the encoder acts as a data compression function. Then, the decoder decompresses xx̂ from the lower-dimensional latent vector, z, where we can think of the decoder as a function, xx̂ = gg(zz). A simple autoencoder architecture is shown in the following figure, where the encoder and decoder parts consist of only one fully connected layer each:\n",
        "        The connection between autoencoders and dimensionality reduction, Compressing Data via Dimensionality Reduction, you learned about dimensionality reduction techniques, such as principal component analysis (PCA) and linear discriminant analysis (LDA). Autoencoders can be used as a dimensionality reduction technique as well. In fact, when there is no nonlinearity in either of the two subnetworks (encoder and decoder), then the autoencoder approach is almost identical to PCA.\n",
        "        Other types of autoencoders based on the size of latent space As previously mentioned, the dimensionality of an autoencoder's latent space is typically lower than the dimensionality of the inputs (p < d), which makes autoencoders suitable for dimensionality reduction. For this reason, the latent vector is also often referred to as the \"bottleneck,\" and this particular configuration of an autoencoder is also called undercomplete. However, there is a different category of autoencoders, called overcomplete, where the dimensionality of the latent vector, z, is, in fact, greater than the dimensionality of the input examples (p > d). When training an overcomplete autoencoder, there is a trivial solution where the encoder and the decoder can simply learn to copy (memorize) the input features to their output layer. Obviously, this solution is not very useful. However, with some modifications to the training procedure, overcomplete autoencoders can be used for noise reduction. In this case, during training, random noise, εε, is added to the input examples and the network learns to reconstruct the clean example, x, from the noisy signal, xx + εε. Then, at evaluation time, we provide the new examples that are naturally noisy (that is, noise is already present such that no additional artificial noise, εε, is added) in order to remove the existing noise from these examples. This particular autoencoder architecture and training method is referred to as a denoising autoencoder.\n",
        "        \"\"\"\n",
        "        ],\n",
        "\n",
        "    'Voting Classifier': [\n",
        "        \"\"\"\n",
        "        An ensemble machine learning model that combines the predictions from multiple other models. It uses majority voting: the prediction label is the one that gets the majority of votes from models.','A voting classifier is a machine learning estimator that trains various base models or estimators and predicts on the basis of aggregating the findings of each base estimator. The aggregating criteria can be combined decision of voting for each estimator output.\n",
        "        A Voting Classifier, also known as an Ensemble Classifier, is a machine learning model that combines the predictions of multiple individual classifiers to make a final prediction. It leverages the wisdom of the crowd by aggregating the predictions of diverse classifiers, resulting in improved overall performance and more robust predictions. The idea behind a Voting Classifier is that different classifiers may have varying strengths and weaknesses, and by combining their predictions, we can overcome individual shortcomings and obtain a more reliable and accurate prediction. The individual classifiers, often referred to as base or component classifiers, can be of different types, such as decision trees, support vector machines, logistic regression, or any other classifier suitable for the given problem.\n",
        "        The idea behind the VotingClassifier is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing models in order to balance out their individual weaknesses. In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.\n",
        "        In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities. Specific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability.\n",
        "        The VotingClassifier can also be used together with GridSearchCV in order to tune the hyperparameters of the individual estimators Voting Classifier supports two types of votings. Hard Voting: In hard voting, the predicted output class is a class with the highest majority of votes i.e the class which had the highest probability of being predicted by each of the classifiers. Suppose three classifiers predicted the output class(A, A, B), so here the majority predicted A as output. Hence A will be the final prediction. Soft Voting: In soft voting, the output class is the prediction based on the average of probability given to that class. Suppose given some input to three models, the prediction probability for class A = (0.30, 0.47, 0.53) and B = (0.20, 0.32, 0.40). So the average for class A is 0.4333 and B is 0.3067, the winner is clearly class A because it had the highest probability averaged by each classifier.\n",
        "\n",
        "        \"\"\"\n",
        "        ],\n",
        "\n",
        "    'ANomaly Detection': [ \n",
        "        \n",
        "    \"\"\"\n",
        "    Isolation Forest An unsupervised learning algorithm for anomaly detection that works on the principle of isolating anomalies, instead of the most common techniques of profiling normal points.', \"Isolation forest is a machine learning algorithm for anomaly detection. It's an unsupervised learning algorithm that identifies anomaly by isolating outliers in the data. Isolation Forest is based on the Decision Tree algorithm.\n",
        "    Isolation Forest is an unsupervised machine learning algorithm used for anomaly detection. It is particularly effective in identifying outliers or anomalies within a dataset, distinguishing them from the majority of normal data points. The algorithm constructs an ensemble of isolation trees to isolate anomalous instances in a quick and efficient manner.The Isolation Forest algorithm takes advantage of the fact that anomalies are typically fewer in number and exhibit different patterns compared to normal data points. It works by randomly selecting a feature and a random split value within the range of that feature. The feature is chosen randomly to prevent biased splits. The data is then partitioned based on whether the feature value of a data point is below or above the chosen split value. This partitioning process continues recursively, creating a tree-like structure until all data points are isolated or a predefined stopping criterion is met.\n",
        "    Local Outlier Factor (LOF) 'An unsupervised outlier detection method which computes the local density deviation of a given data point with respect to its neighbors.','The local outlier factor is based on a concept of a local density, where locality is given by k nearest neighbors, whose distance is used to estimate the density. By comparing the local density of an object to the local densities of its neighbors, one can identify regions of similar density, and points that have a substantially'\n",
        "    Local Outlier Factor (LOF) is an unsupervised machine learning algorithm used for anomaly detection. It is a density-based method that identifies anomalies by measuring the local deviation of data points from their surrounding neighborhoods. LOF takes into account the density distribution of data points to identify outliers that deviate significantly from their local context. The LOF algorithm assesses the anomaly score of each data point based on the concept of local reachability density. For each data point, LOF calculates its local density by estimating the number of neighboring points within a defined distance (known as the neighborhood size). The local reachability density is then computed by comparing the local density of the data point with the local densities of its neighboring points. If a data point has a significantly lower local density compared to its neighbors, it suggests that the point is in a sparser region and could be an outlier.\n",
        "    One-Class SVM An unsupervised algorithm that learns a decision function for novelty detection, classifying between observed and unobserved examples. One-class SVM is an unsupervised algorithm that learns a decision function for novelty detection: classifying new data as similar or different to the training set. One-Class Support Vector Machines (One-Class SVM) is a machine learning algorithm used for unsupervised anomaly detection. It aims to identify anomalies by learning a boundary that encompasses the normal data points while separating them from potential outliers or anomalies. One-Class SVM is particularly useful when only normal data samples are available during training, making it suitable for situations where labeled anomalies are scarce or not available at all. One-Class SVM works by mapping the input data into a high-dimensional feature space and finding a hyperplane that encloses the majority of the data points. The algorithm finds the optimal hyperplane by maximizing the margin around the normal data instances while minimizing the number of data points that lie outside the boundary. In this way, One-Class SVM can capture the characteristics of normal data and identify deviations from it.\n",
        "    \"\"\"\n",
        "]}\n",
        "  \n",
        "                            \n",
        "df = pd.DataFrame.from_dict(data, orient='index', columns=['Description'])\n",
        "                                    \n",
        "                              \n",
        "dfclean = df.replace('\\n','', regex=True)\n"
      ],
      "metadata": {
        "id": "_YD3c_xGjtPa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_continuous = []\n",
        "\n",
        "for index, row in dfclean.iterrows():\n",
        "    data_continuous.append(f\"{index}\\n{row['Description']}\\n\")\n",
        "\n",
        "with open('data.txt', 'w') as file:\n",
        "    file.write(\"\\n\".join(data_continuous))\n"
      ],
      "metadata": {
        "id": "S1wi0_V5Pk4x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments, TextDataset\n",
        "\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=\"data.txt\", # located in the root directory\n",
        "    block_size=64)\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, \n",
        "    mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-model\", # output directory in root\n",
        "    overwrite_output_dir=True, # overwrite the content of the output directory\n",
        "    num_train_epochs=1500, # number of training epochs\n",
        "    learning_rate=5e-6, # learning rate\n",
        "    weight_decay=0.001,\n",
        "\n",
        "    per_device_train_batch_size=32, # batch size for training\n",
        "    save_steps=10_000, # after # steps model is saved \n",
        "    save_total_limit=2, # delete older checkpoints\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "5M0jbiaQsifd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2eAFRZ-VQxvB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "wVmXn-VLvOoS",
        "outputId": "3192cff6-3019-40ee-a43b-d7ed29a8d1c9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6000/6000 49:08, Epoch 1500/1500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.510400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.057600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.379300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.176400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.113500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.088000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.075600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.067700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.064600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.060200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.058600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.058400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6000, training_loss=0.392517396291097, metrics={'train_runtime': 2948.7305, 'train_samples_per_second': 55.448, 'train_steps_per_second': 2.035, 'total_flos': 5340155904000000.0, 'train_loss': 0.392517396291097, 'epoch': 1500.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()\n"
      ],
      "metadata": {
        "id": "4wQhQhq5xPtb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "headliner = pipeline('text-generation',\n",
        "                model='gpt2-model', \n",
        "                tokenizer=tokenizer,\n",
        "                max_new_tokens=200,\n",
        "                \n",
        "                )\n",
        "def get_mlinfo(headliner_pipeline, seed_text=None):\n",
        "  return headliner_pipeline(seed_text)[0]['generated_text'].strip().split('\\n')"
      ],
      "metadata": {
        "id": "IGTtbCi9vvJM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_mlinfo(headliner, seed_text=\"Grid Search does\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLA6OsOyw_25",
        "outputId": "7119d5c8-5f5b-4d44-84a0-2281e42384ae"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Grid Search does not support the use of a single tree to train multiple trees. Instead, each tree is used as the training set for another tree, and the successive trees are used for prediction. In Python, cross-validation is achieved using the cross_val_val() method, which is also a cross-validation method. cross_val() is an integer or floating-point number that can be any number between 0 and 1.',\n",
              " '        If cross-validation is achieved with a value of 1, the error bars represent the error of the original cross-validation. If cross-validation is achieved with a value of 0, the error bars represent the error of the cross-validation. If cross-val_min() is used instead, the default is 1. If cross-val_max() is used, the default is 5. If cross_val() is not enough, the neural network is trained multiple times as input and the appropriate']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input_text = \"Support Vector Machines\"\n",
        "# input_ids = tokenizer.encode(input_text)\n",
        "\n",
        "# # generate text\n",
        "# output = model.generate(input_ids, max_length=150, num_return_sequences=1, temperature=0.9)\n",
        "\n",
        "# for i in range(5):\n",
        "#     print(tokenizer.decode(output[i], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "gDcihK6VRgHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YzeGgEMjx0fW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}